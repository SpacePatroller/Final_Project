{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "#Import Dependencies\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from glob import glob\n",
    "import seaborn as sns\n",
    "from PIL import Image\n",
    "np.random.seed(123)\n",
    "from sklearn.preprocessing import label_binarize\n",
    "from sklearn.metrics import confusion_matrix, cohen_kappa_score, classification_report, precision_recall_curve, roc_auc_score, roc_curve, auc, precision_recall_fscore_support\n",
    "import itertools\n",
    "from scipy import interp\n",
    "from itertools import cycle\n",
    "#import scikitplot as skplt\n",
    "import shutil\n",
    "\n",
    "import keras\n",
    "from keras.utils.np_utils import to_categorical # used for converting labels to one-hot-encoding\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPool2D, MaxPooling2D\n",
    "from keras import backend as K\n",
    "import itertools\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.utils.np_utils import to_categorical # convert to one-hot-encoding\n",
    "\n",
    "from keras.optimizers import Adam\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.callbacks import ReduceLROnPlateau\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from tensorflow import set_random_seed\n",
    "set_random_seed(101)\n",
    "\n",
    "import tensorflow\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.metrics import categorical_crossentropy\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1. Function to plot model's validation loss and validation accuracy\n",
    "def plot_model_history(model_history):\n",
    "    fig, axs = plt.subplots(1,2,figsize=(15,5))\n",
    "    # summarize history for accuracy\n",
    "    axs[0].plot(range(1,len(model_history.history['acc'])+1),model_history.history['acc'])\n",
    "    axs[0].plot(range(1,len(model_history.history['val_acc'])+1),model_history.history['val_acc'])\n",
    "    axs[0].set_title('Model Accuracy')\n",
    "    axs[0].set_ylabel('Accuracy')\n",
    "    axs[0].set_xlabel('Epoch')\n",
    "    axs[0].set_xticks(np.arange(1,len(model_history.history['acc'])+1),len(model_history.history['acc'])/10)\n",
    "    axs[0].legend(['train', 'val'], loc='best')\n",
    "    # summarize history for loss\n",
    "    axs[1].plot(range(1,len(model_history.history['loss'])+1),model_history.history['loss'])\n",
    "    axs[1].plot(range(1,len(model_history.history['val_loss'])+1),model_history.history['val_loss'])\n",
    "    axs[1].set_title('Model Loss')\n",
    "    axs[1].set_ylabel('Loss')\n",
    "    axs[1].set_xlabel('Epoch')\n",
    "    axs[1].set_xticks(np.arange(1,len(model_history.history['loss'])+1),len(model_history.history['loss'])/10)\n",
    "    axs[1].legend(['train', 'val'], loc='best')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_meta_file = os.path.join('resources')\n",
    "image_files = os.path.join('images')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new directory\n",
    "base_dir = 'base_dir'\n",
    "os.mkdir(base_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imageid_path_dict = {os.path.splitext(os.path.basename(x))[0]: x\n",
    "                     for x in glob(os.path.join(image_files, '*.jpg'))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#[CREATE FOLDERS INSIDE THE BASE DIRECTORY]\n",
    "\n",
    "# create a path to 'base_dir' to which we will join the names of the new folders\n",
    "# train_dir\n",
    "train_dir = os.path.join(base_dir, 'train_dir')\n",
    "os.mkdir(train_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# val_dir\n",
    "val_dir = os.path.join(base_dir, 'val_dir')\n",
    "os.mkdir(val_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [CREATE FOLDERS INSIDE THE TRAIN, VALIDATION AND TEST FOLDERS]\n",
    "# Inside each folder we create seperate folders for each class\n",
    "\n",
    "# create new folders inside train_dir\n",
    "nv = os.path.join(train_dir, 'nv')\n",
    "os.mkdir(nv)\n",
    "mel = os.path.join(train_dir, 'mel')\n",
    "os.mkdir(mel)\n",
    "bkl = os.path.join(train_dir, 'bkl')\n",
    "os.mkdir(bkl)\n",
    "bcc = os.path.join(train_dir, 'bcc')\n",
    "os.mkdir(bcc)\n",
    "akiec = os.path.join(train_dir, 'akiec')\n",
    "os.mkdir(akiec)\n",
    "vasc = os.path.join(train_dir, 'vasc')\n",
    "os.mkdir(vasc)\n",
    "df = os.path.join(train_dir, 'df')\n",
    "os.mkdir(df)\n",
    "\n",
    "# Create directories for later 2 category merge\n",
    "ben = os.path.join(train_dir, 'ben')\n",
    "os.mkdir(ben)\n",
    "mal = os.path.join(train_dir, 'mal')\n",
    "os.mkdir(mal)\n",
    "\n",
    "# create new folders inside val_dir\n",
    "nv = os.path.join(val_dir, 'nv')\n",
    "os.mkdir(nv)\n",
    "mel = os.path.join(val_dir, 'mel')\n",
    "os.mkdir(mel)\n",
    "bkl = os.path.join(val_dir, 'bkl')\n",
    "os.mkdir(bkl)\n",
    "bcc = os.path.join(val_dir, 'bcc')\n",
    "os.mkdir(bcc)\n",
    "akiec = os.path.join(val_dir, 'akiec')\n",
    "os.mkdir(akiec)\n",
    "vasc = os.path.join(val_dir, 'vasc')\n",
    "os.mkdir(vasc)\n",
    "df = os.path.join(val_dir, 'df')\n",
    "os.mkdir(df)\n",
    "\n",
    "# Create directories for later 2 category merge\n",
    "\n",
    "ben = os.path.join(val_dir, 'ben')\n",
    "os.mkdir(ben)\n",
    "mal = os.path.join(val_dir, 'mal')\n",
    "os.mkdir(mal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This dictionary is useful for displaying more human-friendly labels later on\n",
    "\n",
    "lesion_type_dict = {\n",
    "    'nv': 'Melanocytic nevi',\n",
    "    'mel': 'Melanoma',\n",
    "    'bkl': 'Benign keratosis-like lesions ',\n",
    "    'bcc': 'Basal cell carcinoma',\n",
    "    'akiec': 'Actinic keratoses',\n",
    "    'vasc': 'Vascular lesions',\n",
    "    'df': 'Dermatofibroma'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataframe to hold metadata\n",
    "df_data = pd.read_csv(os.path.join(base_meta_file, 'HAM10000_metadata.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter for lesion_id's that only have one image\n",
    "\n",
    "# this will tell us how many images are associated with each lesion_id\n",
    "df = df_data.groupby('lesion_id').count()\n",
    "\n",
    "# now we filter out lesion_id's that have only one image associated with it\n",
    "df = df[df['image_id'] == 1]\n",
    "\n",
    "df.reset_index(inplace=True)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# here we identify lesion_id's that have duplicate images and those that have only\n",
    "# one image.\n",
    "\n",
    "def identify_duplicates(x):\n",
    "    \n",
    "    unique_list = list(df['lesion_id'])\n",
    "    \n",
    "    if x in unique_list:\n",
    "        return 'no_duplicates'\n",
    "    else:\n",
    "        return 'has_duplicates'\n",
    "    \n",
    "# create a new colum that is a copy of the lesion_id column\n",
    "df_data['duplicates'] = df_data['lesion_id']\n",
    "# apply the function to this new column\n",
    "df_data['duplicates'] = df_data['duplicates'].apply(identify_duplicates)\n",
    "\n",
    "df_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for duplicate totals\n",
    "df_data['duplicates'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter out images that don't have duplicates\n",
    "df = df_data[df_data['duplicates'] == 'no_duplicates']\n",
    "\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we create a val set using df because we are sure that none of these images\n",
    "# have augmented duplicates in the train set\n",
    "y = df['dx']\n",
    "\n",
    "_, df_val = train_test_split(df, test_size=0.17, random_state=101, stratify=y)\n",
    "\n",
    "df_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This set will be df_data excluding all rows that are in the val set\n",
    "\n",
    "# This function identifies if an image is part of the train\n",
    "# or val set.\n",
    "def identify_val_rows(x):\n",
    "    # create a list of all the lesion_id's in the val set\n",
    "    val_list = list(df_val['image_id'])\n",
    "    \n",
    "    if str(x) in val_list:\n",
    "        return 'val'\n",
    "    else:\n",
    "        return 'train'\n",
    "\n",
    "# identify train and val rows\n",
    "\n",
    "# create a new colum that is a copy of the image_id column\n",
    "df_data['train_or_val'] = df_data['image_id']\n",
    "# apply the function to this new column\n",
    "df_data['train_or_val'] = df_data['train_or_val'].apply(identify_val_rows)\n",
    "   \n",
    "# filter out train rows\n",
    "df_train = df_data[df_data['train_or_val'] == 'train']\n",
    "\n",
    "\n",
    "print(len(df_train))\n",
    "print(len(df_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['dx'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data[\"image_id_stepwise\"] = df_data[\"image_id\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the image_id as the index in df_data\n",
    "df_data.set_index('image_id', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a list of images in each of the two folders\n",
    "folder_1 = os.listdir('images')\n",
    "\n",
    "# Get a list of train and val images\n",
    "train_list = list(df_train['image_id'])\n",
    "val_list = list(df_val['image_id'])\n",
    "\n",
    "# Transfer the train images\n",
    "\n",
    "for image in train_list:\n",
    "    \n",
    "    fname = image + '.jpg'\n",
    "    label = df_data.loc[image,'dx']\n",
    "    \n",
    "    if fname in folder_1:\n",
    "        # source path to image\n",
    "        src = os.path.join('images', fname)\n",
    "        # destination path to image\n",
    "        dst = os.path.join(train_dir, label, fname)\n",
    "        # copy the image from the source to the destination\n",
    "        shutil.copyfile(src, dst)\n",
    "\n",
    "# Transfer the val images\n",
    "\n",
    "for image in val_list:\n",
    "    \n",
    "    fname = image + '.jpg'\n",
    "    label = df_data.loc[image,'dx']\n",
    "    \n",
    "    if fname in folder_1:\n",
    "        # source path to image\n",
    "        src = os.path.join('images', fname)\n",
    "        # destination path to image\n",
    "        dst = os.path.join(val_dir, label, fname)\n",
    "        # copy the image from the source to the destination\n",
    "        shutil.copyfile(src, dst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check how many train images we have in each folder\n",
    "\n",
    "print(len(os.listdir('base_dir/train_dir/nv')))\n",
    "print(len(os.listdir('base_dir/train_dir/mel')))\n",
    "print(len(os.listdir('base_dir/train_dir/bkl')))\n",
    "print(len(os.listdir('base_dir/train_dir/bcc')))\n",
    "print(len(os.listdir('base_dir/train_dir/akiec')))\n",
    "print(len(os.listdir('base_dir/train_dir/vasc')))\n",
    "print(len(os.listdir('base_dir/train_dir/df')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check how many val images we have in each folder\n",
    "\n",
    "print(len(os.listdir('base_dir/val_dir/nv')))\n",
    "print(len(os.listdir('base_dir/val_dir/mel')))\n",
    "print(len(os.listdir('base_dir/val_dir/bkl')))\n",
    "print(len(os.listdir('base_dir/val_dir/bcc')))\n",
    "print(len(os.listdir('base_dir/val_dir/akiec')))\n",
    "print(len(os.listdir('base_dir/val_dir/vasc')))\n",
    "print(len(os.listdir('base_dir/val_dir/df')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# note that we are not augmenting class 'nv'\n",
    "class_list = ['mel','bkl','bcc','akiec','vasc','df']\n",
    "\n",
    "for item in class_list:\n",
    "    \n",
    "    # We are creating temporary directories here because we delete these directories later\n",
    "    # create a base dir\n",
    "    aug_dir = 'aug_dir'\n",
    "    os.mkdir(aug_dir)\n",
    "    # create a dir within the base dir to store images of the same class\n",
    "    img_dir = os.path.join(aug_dir, 'img_dir')\n",
    "    os.mkdir(img_dir)\n",
    "\n",
    "    # Choose a class\n",
    "    img_class = item\n",
    "\n",
    "    # list all images in that directory\n",
    "    img_list = os.listdir('base_dir/train_dir/' + img_class)\n",
    "\n",
    "    # Copy images from the class train dir to the img_dir e.g. class 'mel'\n",
    "    for fname in img_list:\n",
    "            # source path to image\n",
    "            src = os.path.join('base_dir/train_dir/' + img_class, fname)\n",
    "            # destination path to image\n",
    "            dst = os.path.join(img_dir, fname)\n",
    "            # copy the image from the source to the destination\n",
    "            shutil.copyfile(src, dst)\n",
    "\n",
    "    # point to a dir containing the images and not to the images themselves\n",
    "    path = aug_dir\n",
    "    save_path = 'base_dir/train_dir/' + img_class\n",
    "\n",
    "    # Create a data generator\n",
    "    datagen = ImageDataGenerator(\n",
    "        rotation_range=180,\n",
    "        width_shift_range=0.1,\n",
    "        height_shift_range=0.1,\n",
    "        zoom_range=0.1,\n",
    "        horizontal_flip=True,\n",
    "        vertical_flip=True,\n",
    "        #brightness_range=(0.9,1.1),\n",
    "        fill_mode='nearest')\n",
    "        #zca_whitening=False)  # apply ZCA whitening\n",
    "\n",
    "    batch_size = 50\n",
    "\n",
    "    aug_datagen = datagen.flow_from_directory(path,\n",
    "                            save_to_dir=save_path,\n",
    "                            save_format='jpg',\n",
    "                            target_size=(224,224),\n",
    "                            batch_size=batch_size)\n",
    "\n",
    "    # Generate the augmented images and add them to the training folders\n",
    "    ###########\n",
    "    num_aug_images_wanted = 10000 # total number of images we want to have in each class\n",
    "    ###########\n",
    "    \n",
    "    num_files = len(os.listdir(img_dir))\n",
    "    num_batches = int(np.ceil((num_aug_images_wanted-num_files)/batch_size))\n",
    "\n",
    "    # run the generator and create about 10000 augmented images\n",
    "    for i in range(0,num_batches):\n",
    "\n",
    "        imgs, labels = next(aug_datagen)\n",
    "        \n",
    "    # delete temporary directory with the raw image files\n",
    "    shutil.rmtree('aug_dir')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check how many train images we now have in each folder.\n",
    "# This is the original images plus the augmented images.\n",
    "\n",
    "print(len(os.listdir('base_dir/train_dir/nv')))\n",
    "print(len(os.listdir('base_dir/train_dir/mel')))\n",
    "print(len(os.listdir('base_dir/train_dir/bkl')))\n",
    "print(len(os.listdir('base_dir/train_dir/bcc')))\n",
    "print(len(os.listdir('base_dir/train_dir/akiec')))\n",
    "print(len(os.listdir('base_dir/train_dir/vasc')))\n",
    "print(len(os.listdir('base_dir/train_dir/df')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check how many val images we have in each folder.\n",
    "print(len(os.listdir('base_dir/val_dir/nv')))\n",
    "print(len(os.listdir('base_dir/val_dir/mel')))\n",
    "print(len(os.listdir('base_dir/val_dir/bkl')))\n",
    "print(len(os.listdir('base_dir/val_dir/bcc')))\n",
    "print(len(os.listdir('base_dir/val_dir/akiec')))\n",
    "print(len(os.listdir('base_dir/val_dir/vasc')))\n",
    "print(len(os.listdir('base_dir/val_dir/df')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Move all files from training and validation directories into binary classes\n",
    "#  ben = benign, represented later by 0's, \n",
    "#  mal = malignant, represented later by 1's\n",
    "\n",
    "# Move augmented benign train files\n",
    "\n",
    "src = 'base_dir/train_dir/nv'\n",
    "dest = 'base_dir/train_dir/ben'\n",
    "src_files = os.listdir(src)\n",
    "for file_name in src_files:\n",
    "    full_file_name = os.path.join(src, file_name)\n",
    "    if (os.path.isfile(full_file_name)):\n",
    "        shutil.move(full_file_name, dest)\n",
    "        \n",
    "src = 'base_dir/train_dir/bkl'\n",
    "dest = 'base_dir/train_dir/ben'\n",
    "src_files = os.listdir(src)\n",
    "for file_name in src_files:\n",
    "    full_file_name = os.path.join(src, file_name)\n",
    "    if (os.path.isfile(full_file_name)):\n",
    "        shutil.move(full_file_name, dest)\n",
    "\n",
    "src = 'base_dir/train_dir/vasc'\n",
    "dest = 'base_dir/train_dir/ben'\n",
    "src_files = os.listdir(src)\n",
    "for file_name in src_files:\n",
    "    full_file_name = os.path.join(src, file_name)\n",
    "    if (os.path.isfile(full_file_name)):\n",
    "        shutil.move(full_file_name, dest)\n",
    "\n",
    "src = 'base_dir/train_dir/df'\n",
    "dest = 'base_dir/train_dir/ben'\n",
    "src_files = os.listdir(src)\n",
    "for file_name in src_files:\n",
    "    full_file_name = os.path.join(src, file_name)\n",
    "    if (os.path.isfile(full_file_name)):\n",
    "        shutil.move(full_file_name, dest)\n",
    "        \n",
    "# Move augmented malignant train files\n",
    "\n",
    "src = 'base_dir/train_dir/mel'\n",
    "dest = 'base_dir/train_dir/mal'\n",
    "src_files = os.listdir(src)\n",
    "for file_name in src_files:\n",
    "    full_file_name = os.path.join(src, file_name)\n",
    "    if (os.path.isfile(full_file_name)):\n",
    "        shutil.move(full_file_name, dest)\n",
    "\n",
    "src = 'base_dir/train_dir/bcc'\n",
    "dest = 'base_dir/train_dir/mal'\n",
    "src_files = os.listdir(src)\n",
    "for file_name in src_files:\n",
    "    full_file_name = os.path.join(src, file_name)\n",
    "    if (os.path.isfile(full_file_name)):\n",
    "        shutil.move(full_file_name, dest)\n",
    "\n",
    "src = 'base_dir/train_dir/akiec'\n",
    "dest = 'base_dir/train_dir/mal'\n",
    "src_files = os.listdir(src)\n",
    "for file_name in src_files:\n",
    "    full_file_name = os.path.join(src, file_name)\n",
    "    if (os.path.isfile(full_file_name)):\n",
    "        shutil.move(full_file_name, dest)\n",
    "        \n",
    "# Move augmented malignant validation files\n",
    "\n",
    "src = 'base_dir/val_dir/nv'\n",
    "dest = 'base_dir/val_dir/ben'\n",
    "src_files = os.listdir(src)\n",
    "for file_name in src_files:\n",
    "    full_file_name = os.path.join(src, file_name)\n",
    "    if (os.path.isfile(full_file_name)):\n",
    "        shutil.move(full_file_name, dest)\n",
    "        \n",
    "src = 'base_dir/val_dir/bkl'\n",
    "dest = 'base_dir/val_dir/ben'\n",
    "src_files = os.listdir(src)\n",
    "for file_name in src_files:\n",
    "    full_file_name = os.path.join(src, file_name)\n",
    "    if (os.path.isfile(full_file_name)):\n",
    "        shutil.move(full_file_name, dest)\n",
    "\n",
    "src = 'base_dir/val_dir/vasc'\n",
    "dest = 'base_dir/val_dir/ben'\n",
    "src_files = os.listdir(src)\n",
    "for file_name in src_files:\n",
    "    full_file_name = os.path.join(src, file_name)\n",
    "    if (os.path.isfile(full_file_name)):\n",
    "        shutil.move(full_file_name, dest)\n",
    "\n",
    "src = 'base_dir/val_dir/df'\n",
    "dest = 'base_dir/val_dir/ben'\n",
    "src_files = os.listdir(src)\n",
    "for file_name in src_files:\n",
    "    full_file_name = os.path.join(src, file_name)\n",
    "    if (os.path.isfile(full_file_name)):\n",
    "        shutil.move(full_file_name, dest)\n",
    "        \n",
    "# Move augmented malignant validation files\n",
    "\n",
    "src = 'base_dir/val_dir/mel'\n",
    "dest = 'base_dir/val_dir/mal'\n",
    "src_files = os.listdir(src)\n",
    "for file_name in src_files:\n",
    "    full_file_name = os.path.join(src, file_name)\n",
    "    if (os.path.isfile(full_file_name)):\n",
    "        shutil.move(full_file_name, dest)\n",
    "\n",
    "src = 'base_dir/val_dir/bcc'\n",
    "dest = 'base_dir/val_dir/mal'\n",
    "src_files = os.listdir(src)\n",
    "for file_name in src_files:\n",
    "    full_file_name = os.path.join(src, file_name)\n",
    "    if (os.path.isfile(full_file_name)):\n",
    "        shutil.move(full_file_name, dest)\n",
    "\n",
    "src = 'base_dir/val_dir/akiec'\n",
    "dest = 'base_dir/val_dir/mal'\n",
    "src_files = os.listdir(src)\n",
    "for file_name in src_files:\n",
    "    full_file_name = os.path.join(src, file_name)\n",
    "    if (os.path.isfile(full_file_name)):\n",
    "        shutil.move(full_file_name, dest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove the old augmentation directories\n",
    "\n",
    "shutil.rmtree('base_dir/train_dir/akiec')\n",
    "shutil.rmtree('base_dir/train_dir/bcc')\n",
    "shutil.rmtree('base_dir/train_dir/bkl')\n",
    "shutil.rmtree('base_dir/train_dir/df')\n",
    "shutil.rmtree('base_dir/train_dir/mel')\n",
    "shutil.rmtree('base_dir/train_dir/nv')\n",
    "shutil.rmtree('base_dir/train_dir/vasc')\n",
    "\n",
    "shutil.rmtree('base_dir/val_dir/akiec')\n",
    "shutil.rmtree('base_dir/val_dir/bcc')\n",
    "shutil.rmtree('base_dir/val_dir/bkl')\n",
    "shutil.rmtree('base_dir/val_dir/df')\n",
    "shutil.rmtree('base_dir/val_dir/mel')\n",
    "shutil.rmtree('base_dir/val_dir/nv')\n",
    "shutil.rmtree('base_dir/val_dir/vasc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check how many val images we have in each folder.\n",
    "print(len(os.listdir('base_dir/train_dir/mal')))\n",
    "print(len(os.listdir('base_dir/train_dir/ben')))\n",
    "print(len(os.listdir('base_dir/val_dir/mal')))\n",
    "print(len(os.listdir('base_dir/val_dir/ben')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data.head()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# For Stepwise - Create New Columns for better readability\n",
    "\n",
    "df_data['path'] = df_data['image_id_stepwise'].map(imageid_path_dict.get)\n",
    "df_data['cell_type'] = df_data['dx'].map(lesion_type_dict.get)\n",
    "df_data['malignant_benign'] = df_data['dx']\n",
    "df_data.loc[df_data['malignant_benign'] == 'mel', 'malignant_benign'] = 1\n",
    "df_data.loc[df_data['malignant_benign'] == 'bcc', 'malignant_benign'] = 1\n",
    "df_data.loc[df_data['malignant_benign'] == 'akiec', 'malignant_benign'] = 1\n",
    "df_data.loc[df_data['malignant_benign'] == 'nv', 'malignant_benign'] = 0\n",
    "df_data.loc[df_data['malignant_benign'] == 'bkl', 'malignant_benign'] = 0\n",
    "df_data.loc[df_data['malignant_benign'] == 'vasc', 'malignant_benign'] = 0\n",
    "df_data.loc[df_data['malignant_benign'] == 'df', 'malignant_benign'] = 0\n",
    "df_data['cell_type_idx'] = pd.Categorical(df_data['cell_type']).codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_data['age'].fillna((df_data['age'].mean()), inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(df_data.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup model-related variables\n",
    "\n",
    "train_path = 'base_dir/train_dir'\n",
    "valid_path = 'base_dir/val_dir'\n",
    "\n",
    "num_train_samples = len(df_train)\n",
    "num_val_samples = len(df_val)\n",
    "train_batch_size = 20\n",
    "val_batch_size = 20\n",
    "image_size = 224\n",
    "\n",
    "train_steps = np.ceil(num_train_samples / train_batch_size)\n",
    "val_steps = np.ceil(num_val_samples / val_batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# More model setup\n",
    "\n",
    "datagen = ImageDataGenerator(\n",
    "    preprocessing_function= \\\n",
    "    tensorflow.keras.applications.mobilenet.preprocess_input)\n",
    "\n",
    "train_batches = datagen.flow_from_directory(train_path,\n",
    "                                            target_size=(image_size,image_size),\n",
    "                                            batch_size=train_batch_size)\n",
    "\n",
    "valid_batches = datagen.flow_from_directory(valid_path,\n",
    "                                            target_size=(image_size,image_size),\n",
    "                                            batch_size=val_batch_size)\n",
    "\n",
    "# Note: shuffle=False causes the test dataset to not be shuffled\n",
    "test_batches = datagen.flow_from_directory(valid_path,\n",
    "                                            target_size=(image_size,image_size),\n",
    "                                            batch_size=1,\n",
    "                                            shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check how many val images we have in each folder.\n",
    "print(len(os.listdir('base_dir/train_dir/ben')))\n",
    "print(len(os.listdir('base_dir/train_dir/mal')))\n",
    "print(len(os.listdir('base_dir/val_dir/ben')))\n",
    "print(len(os.listdir('base_dir/val_dir/mal')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a copy of a mobilenet model\n",
    "\n",
    "mobile = tensorflow.keras.applications.mobilenet.MobileNet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Summary\n",
    "#mobile.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many layers does MobileNet have?\n",
    "len(mobile.layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CREATE THE MODEL ARCHITECTURE\n",
    "\n",
    "# Exclude the last 5 layers of the above model.\n",
    "# This will include all layers up to and including global_average_pooling2d_1\n",
    "x = mobile.layers[-6].output\n",
    "\n",
    "# Create a new dense layer for predictions\n",
    "# 2 corresponds to the number of classes\n",
    "x = Dropout(0.25)(x)\n",
    "predictions = Dense(2, activation='sigmoid')(x)\n",
    "\n",
    "# inputs=mobile.input selects the input layer, outputs=predictions refers to the\n",
    "# dense layer we created above.\n",
    "\n",
    "model = Model(inputs=mobile.input, outputs=predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Updated model summary\n",
    "#model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We need to choose how many layers we actually want to be trained.\n",
    "\n",
    "# Here we are freezing the weights of all layers except the\n",
    "# last 23 layers in the new model.\n",
    "# The last 23 layers of the model will be trained.\n",
    "\n",
    "for layer in model.layers[:-23]:\n",
    "    layer.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(Adam(lr=0.01), loss='binary_crossentropy', \n",
    "              metrics=['binary_accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the labels that are associated with each index\n",
    "print(valid_batches.class_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add weights to try to make the model more sensitive\n",
    "\n",
    "class_weights={\n",
    "    0: 1.0, # ben\n",
    "    1: 3.0, # mal #Try to make the model more sensitive to malignant\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Set up final model variables\n",
    "\n",
    "filepath = \"model_binary.h5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_binary_accuracy', verbose=1, \n",
    "                             save_best_only=True, mode='max')\n",
    "\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_binary_accuracy', factor=0.5, patience=5, \n",
    "                                   verbose=1, mode='max', min_lr=0.00001)\n",
    "                              \n",
    "                              \n",
    "callbacks_list = [checkpoint, reduce_lr]\n",
    "\n",
    "history = model.fit_generator(train_batches, steps_per_epoch=train_steps, \n",
    "                              class_weight=class_weights,\n",
    "                    validation_data=valid_batches,\n",
    "                    validation_steps=val_steps,\n",
    "                    epochs=20, verbose=1,\n",
    "                   callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the metric names so we can use evaulate_generator\n",
    "model.metrics_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here the the last epoch will be used.\n",
    "\n",
    "val_loss, val_cat_acc = \\\n",
    "model.evaluate_generator(test_batches, \n",
    "                        steps=len(df_val))\n",
    "\n",
    "print('val_loss:', val_loss)\n",
    "print('val_cat_acc:', val_cat_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here the best epoch will be used.\n",
    "\n",
    "model.load_weights('model_binary.h5')\n",
    "\n",
    "val_loss, val_cat_acc = \\\n",
    "model.evaluate_generator(test_batches, \n",
    "                        steps=len(df_val))\n",
    "\n",
    "print('val_loss:', val_loss)\n",
    "print('val_cat_acc:', val_cat_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# display the loss and accuracy curves\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "acc = history.history['binary accuracy']\n",
    "val_acc = history.history['val_accuracy']\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "epochs = range(1, len(acc) + 1)\n",
    "\n",
    "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
    "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.legend()\n",
    "plt.figure()\n",
    "\n",
    "plt.plot(epochs, acc, 'bo', label='Training cat acc')\n",
    "plt.plot(epochs, val_acc, 'b', label='Validation cat acc')\n",
    "plt.title('Training and validation cat accuracy')\n",
    "plt.legend()\n",
    "plt.figure()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the labels of the test images.\n",
    "test_labels = test_batches.classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the label associated with each class\n",
    "test_batches.class_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a prediction\n",
    "predictions = model.predict_generator(test_batches, steps=len(df_val), verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Source: Scikit Learn website\n",
    "# http://scikit-learn.org/stable/auto_examples/\n",
    "# model_selection/plot_confusion_matrix.html#sphx-glr-auto-examples-model-\n",
    "# selection-plot-confusion-matrix-py\n",
    "\n",
    "\n",
    "def plot_confusion_matrix(cm, test_labels,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    print(cm)\n",
    "\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(test_labels))\n",
    "    plt.xticks(tick_marks, test_labels, rotation=45)\n",
    "    plt.yticks(tick_marks, test_labels)\n",
    "\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# argmax returns the index of the max value in a row\n",
    "cm = confusion_matrix(test_labels, predictions.argmax(axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_batches.class_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Define the labels of the class indices. These need to match the \n",
    "# order shown above.\n",
    "cm_plot_labels = ['ben', 'mal']\n",
    "\n",
    "plot_confusion_matrix(cm, cm_plot_labels, title='Confusion Matrix')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the index of the class with the highest probability score\n",
    "y_pred = np.argmax(predictions, axis=1)\n",
    "\n",
    "# Get the labels of the test images.\n",
    "y_true = test_batches.classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Generate a classification report\n",
    "report = classification_report(y_true, y_pred, target_names=cm_plot_labels)\n",
    "\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cohen_kappa_score(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "roc_auc = roc_auc_score(y_true, y_pred) \n",
    "roc_auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpr, tpr, _ = roc_curve(y_true,  y_pred)\n",
    "auc = roc_auc_score(y_true, y_pred)\n",
    "plt.plot(fpr,tpr,label=\"Predictor, auc=\"+str(auc))\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.xlabel('True Negative Rate')\n",
    "plt.legend(loc=4)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To clear model input if needed\n",
    "#model = None"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
